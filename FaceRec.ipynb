{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "HW2P2_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og2OkfxDS13Y"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGS3D4opS13b",
        "outputId": "a1f968b3-8f42-4acd-b355-06e8a00ff481"
      },
      "source": [
        "cuda = True if torch.cuda.is_available() else False\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.set_device(device)\n",
        "print(cuda)\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwE2LrdyS13f"
      },
      "source": [
        "# using a pre-process transformation for resizing and normalizing\n",
        "pre_process = transforms.Compose([transforms.Resize(256), \n",
        "                                  transforms.CenterCrop(224), \n",
        "                                  transforms.ToTensor(), \n",
        "                                  transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                                                       std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "#creating data loaders for training and validation data\n",
        "ImageData = torchvision.datasets.ImageFolder(root = 'classification_data/train_data', transform = pre_process)\n",
        "ImageLoader = DataLoader(ImageData, batch_size=64, shuffle=True, num_workers=4)\n",
        "\n",
        "ValData = torchvision.datasets.ImageFolder(root = 'classification_data/val_data', transform = pre_process)\n",
        "ValLoader = DataLoader(ValData, batch_size=64, shuffle=True, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfjJCNW7S13h",
        "outputId": "136b041f-d28c-4f17-e091-9a421e035b09"
      },
      "source": [
        "print(\"train data length: {}, classes: {}\".format(ImageData.__len__(), len(ImageData.classes)))\n",
        "print(\"val data length: {}, classes: {}\".format(ValData.__len__(), len(ValData.classes)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data length: 380638, classes: 4000\n",
            "val data length: 8000, classes: 4000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EToCy_8DS13o"
      },
      "source": [
        "# Basic Block for resnet\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, channels_1, channels_2, stride):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(channels_1, channels_2, kernel_size=3, stride=self.stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(channels_2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(channels_2, channels_2, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(channels_2)\n",
        "        )\n",
        "        \n",
        "        # use an identity skip connection \n",
        "        self.shortcut = nn.Sequential(\n",
        "            nn.Conv2d(channels_1, channels_2, kernel_size=1, stride=self.stride, bias=False),\n",
        "            nn.BatchNorm2d(channels_2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_out = self.block(x)\n",
        "        if self.stride == 2:\n",
        "            x_out += self.shortcut(x)\n",
        "        return F.relu(x_out)\n",
        "\n",
        "#xavier initialization\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_normal_(m.weight.data)\n",
        "\n",
        "#resnet 18 architecture following pytorch implementation\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, in_channels, classes):\n",
        "        super(Network, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.classes = classes\n",
        "        self.layer_seq = nn.Sequential(            \n",
        "            nn.Conv2d(in_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            BasicBlock(64, 64, 1),\n",
        "            BasicBlock(64, 64, 1),            \n",
        "            BasicBlock(64, 128, 2),\n",
        "            BasicBlock(128, 128, 1),\n",
        "            BasicBlock(128, 256, 2),\n",
        "            BasicBlock(256, 256, 1),\n",
        "            BasicBlock(256, 512, 2),\n",
        "            BasicBlock(512, 512, 1)\n",
        "        )\n",
        "        self.avgPool2d = nn.AvgPool2d(4)\n",
        "        self.linear = nn.Linear(512, classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_seq(x)\n",
        "        x = self.avgPool2d(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.linear(x)\n",
        "    \n",
        "    def verify_forward(self, x):\n",
        "        x = self.layer_seq(x)\n",
        "        x = self.avgPool2d(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x    \n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_normal_(m.weight.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVq9MKlcS13q"
      },
      "source": [
        "# defining model, criteria and optimizer\n",
        "model = Network(3, 4000)\n",
        "model.apply(init_weights)\n",
        "model.to(device)\n",
        "criteria = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.15, weight_decay = 5e-5, momentum = 0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqcgDP5tS13t"
      },
      "source": [
        "import time\n",
        "def train (model, ImageLoader, criterion, optimizer):\n",
        "    model.train()\n",
        "    current_loss = 0\n",
        "    current_correct = 0\n",
        "    total = 0\n",
        "    for batch, (images, labels) in enumerate(ImageLoader):\n",
        "        if (batch+1)%500 == 0:\n",
        "            print(\"batch: {} | accuracy: {}\".format(batch+1, acc))\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "#         output = model.forward(images)\n",
        "        output = model(images)\n",
        "        y_prob = torch.argmax(output, dim = 1).to(device)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss += loss.item()*images.size(0)\n",
        "        current_correct += (y_prob == labels).sum().item()\n",
        "        total += len(labels)\n",
        "        acc = current_correct/total\n",
        "    epoch_loss = current_loss /len(ImageLoader.dataset)\n",
        "    epoch_acc = acc\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model, ValLoader):\n",
        "    model.eval()\n",
        "    current_correct = 0\n",
        "    total = 0\n",
        "    for batch, (images, labels) in enumerate(ValLoader):\n",
        "        if (batch+1)%500 == 0:\n",
        "            print(\"batch: {} | accuracy: {}\".format(batch+1, acc))\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(images)\n",
        "            y_prob = torch.argmax(output, dim = 1).to(device)\n",
        "            current_correct += (y_prob == labels).sum().item()\n",
        "        total += len(labels)\n",
        "        acc = current_correct/total\n",
        "    epoch_acc = acc\n",
        "    print(\"val accuracy: {}\".format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QerbkBS-S13v"
      },
      "source": [
        "# trained for 15 epochs\n",
        "epochs = 15\n",
        "for epoch in range(epochs):\n",
        "    if epoch >= 1:\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = 0.85 * param_group['lr']\n",
        "    start = time.time()\n",
        "    epoch_train_loss, epoch_train_acc = train(model, ImageLoader, criteria, optimizer)\n",
        "    print(\"Epoch: {}, Time: {}\".format(epoch+1, int(time.time()-start)))\n",
        "    print(\"Epoch: {} Train Loss : {:.4f}  Train Accuracy: {:.4f}\".format(epoch+1,epoch_train_loss,epoch_train_acc))\n",
        "    state = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch}\n",
        "    torch.save(state, \"HW2P2_classification_resnet18_epoch15.pt\")\n",
        "    if (epoch+1)%2 == 0:\n",
        "        model.eval()\n",
        "        validate(model, ValLoader)\n",
        "        model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ldk9z07ZS13x"
      },
      "source": [
        "# data loader for verification data\n",
        "VerificationData = torchvision.datasets.ImageFolder(root = 'verification', transform = pre_process)\n",
        "VerificationLoader = DataLoader(VerificationData, batch_size=128, shuffle=False, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vub33hkjS13z",
        "outputId": "380287ca-db8f-44d7-bfef-1a8c2d6a6d9d"
      },
      "source": [
        "print(\"verification data length: {}, classes: {}\".format(len(VerificationData), len(VerificationData.classes)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "verification data length: 69097, classes: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_27Q0dnS131"
      },
      "source": [
        "# using modified forward method in model for generating embeddings (removed linear layer)\n",
        "def embeddings(model, VerificationLoader):\n",
        "    embeddings = None\n",
        "    model.eval()\n",
        "    current_correct = 0\n",
        "    total = 0\n",
        "    for batch, (images, labels) in enumerate(VerificationLoader):\n",
        "        images = images.to(device)\n",
        "        with torch.no_grad():\n",
        "            if batch == 0:\n",
        "                embeddings = model.verify_forward(images)\n",
        "            else:\n",
        "                embeddings = torch.cat((embeddings, model.verify_forward(images)), dim=0)\n",
        "        del images\n",
        "        torch.cuda.empty_cache()\n",
        "    return embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fjleJaES133"
      },
      "source": [
        "# loading checkpointed model\n",
        "checkpoint = torch.load(\"HW2P2_classification_resnet18_epoch15.pt\")\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "epoch_start = checkpoint['epoch']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co8WFFjcS135",
        "outputId": "5bea9d71-efc5-4328-e638-51be11b45566"
      },
      "source": [
        "# generate embeddings\n",
        "embeddings = embeddings(model, VerificationLoader)\n",
        "print(embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([69097, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEmgq3IMS138",
        "outputId": "a6978604-1e75-4512-a468-38e40618f5a3"
      },
      "source": [
        "# creating dict for mapping embeddings to image names\n",
        "embed_dict = {VerificationData.imgs[i][0][13:]: embeddings[i] for i in range(len(VerificationData))}\n",
        "print(len(embed_dict))\n",
        "torch.save(embed_dict, \"embed_dict.pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYlht4xKS13-",
        "outputId": "968fe421-28b2-44b1-e51b-0abbfe8d9731"
      },
      "source": [
        "embed_dict = torch.load(\"embed_dict.pt\") \n",
        "print(len(embed_dict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BcVdgiGS14A",
        "outputId": "6ce672e6-2f8e-4c19-aa34-93b1aa2c94ed"
      },
      "source": [
        "# creating list of verification images pair names\n",
        "verification_images = open(\"verification_pairs_val.txt\").read().splitlines()\n",
        "verification_images = [line.split() for line in verification_images]\n",
        "print(verification_images[:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['verification_data/00041961.jpg', 'verification_data/00044353.jpg', '0'], ['verification_data/00007133.jpg', 'verification_data/00060449.jpg', '1'], ['verification_data/00041961.jpg', 'verification_data/00020166.jpg', '0'], ['verification_data/00013102.jpg', 'verification_data/00055525.jpg', '1'], ['verification_data/00002921.jpg', 'verification_data/00041331.jpg', '0']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ5TyXALS14C"
      },
      "source": [
        "# creating data loader for test data\n",
        "TestData = torchvision.datasets.ImageFolder(root = 'classification_data/test_data', transform = pre_process)\n",
        "TestLoader = DataLoader(TestData, batch_size=64, shuffle=False, num_workers=4)\n",
        "print(len(TestData))\n",
        "validate(model, TestLoader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a3r2v4oS14F",
        "outputId": "f4b39904-1354-4248-bb95-932225d581b5"
      },
      "source": [
        "# using cosine similarity measure to compute distance between images\n",
        "cos = nn.CosineSimilarity(dim = 0)\n",
        "\n",
        "similarity_scores = np.array([cos(embed_dict[verification_images[i][0]], \n",
        "                                  embed_dict[verification_images[i][1]]).item() \n",
        "                              for i in range(len(verification_images))]) \n",
        "\n",
        "print(len(verification_images) == len(similarity_scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LThtakepS14H"
      },
      "source": [
        "labels = np.array([int(verification_images[i][2]) for i in range(len(verification_images))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcdsyok7S14J",
        "outputId": "3f5968ba-9632-4758-d0a2-ad53aef5ed8a"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "print(len(labels))\n",
        "print(type(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8805\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TXL0MrrS14M",
        "outputId": "ddfc91cf-eec1-4e58-99ed-8666cf62a7be"
      },
      "source": [
        "print(roc_auc_score(labels, similarity_scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9345160740876468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQVPSxFIS14O",
        "outputId": "360cf7b4-b62b-4d8e-9c40-b516d2c7a5f3"
      },
      "source": [
        "test_images = open(\"verification_pairs_test.txt\").read().splitlines()\n",
        "test_images_separated = [line.split() for line in test_images]\n",
        "print(test_images[:2])\n",
        "print(test_images_separated[:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['verification_data/00020839.jpg verification_data/00035322.jpg', 'verification_data/00002921.jpg verification_data/00021567.jpg']\n",
            "[['verification_data/00020839.jpg', 'verification_data/00035322.jpg'], ['verification_data/00002921.jpg', 'verification_data/00021567.jpg']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjBXsBL7S14R",
        "outputId": "a0355535-c18f-48c8-d93c-50986bf65624"
      },
      "source": [
        "test_similarity_scores = [cos(embed_dict[test_images_separated[i][0]], \n",
        "                                  embed_dict[test_images_separated[i][1]]).item() \n",
        "                              for i in range(len(test_images_separated))]\n",
        "\n",
        "print(len(test_images_separated) == len(test_similarity_scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASC8I68_S14U"
      },
      "source": [
        "import pandas as pd\n",
        "results = pd.DataFrame(list(zip(test_images, test_similarity_scores)), \n",
        "               columns =['Id', 'Category']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPRKeM94S14W",
        "outputId": "0297525a-5b51-4c48-bb89-ec3af03d770c"
      },
      "source": [
        "print(results.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  Id  Category\n",
            "0  verification_data/00020839.jpg verification_da...  0.764679\n",
            "1  verification_data/00002921.jpg verification_da...  0.364209\n",
            "2  verification_data/00011732.jpg verification_da...  0.409506\n",
            "3  verification_data/00052778.jpg verification_da...  0.510879\n",
            "4  verification_data/00053948.jpg verification_da...  0.640916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT0Cmmx0S14Z"
      },
      "source": [
        "results.to_csv('results_1.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}